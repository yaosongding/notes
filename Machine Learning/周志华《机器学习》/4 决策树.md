# 决策树

## 基本流程

- 三种导致递归返回（不能再划分）的情况（ p74），并注意作者强调的它们之间的区别

## 划分选择

**三种策略：**

- **信息增益**
- **增益率**
- **基尼指数**

1. 信息增益
   - 简单来说就是分开后的集合的信息熵比分开前的集合减少了多少，减少的越多，信息增益越大。
   - 信息熵：越小纯度越高，最小值为0（所有样本都属于同一类），最大值为$log_2|\gamma|$（每个样本都各自属于一类）

2. 增益率

   - **有了信息增益，为什么还要使用增益率？**

     作者用“编号”作为特征的情况举例，说明了信息增益准则对可取值数目较多的属性有所偏好。增益率就是为了减少这种偏好带来的不利影响。

   - 增益率即信息增益除以IV(a)，其中a是某一个属性。属性a的可能取值越多，IV(a)就会越大

   - 值得注意的是，**增益率准则对可取值数目较少的属性有偏好**

   - 书中也说明了C4.5算法对划分选择的策略：

     ![image-20181207140113594](/Users/yaosongding/Library/Application Support/typora-user-images/image-20181207140113594.png)

3. 基尼指数
   - CART（分类和回归都可用的决策树算法）算法使用的划分属性选择策略
   - gini index反映了从数据集D中随机抽取两个样本，其类别不一致的概率。所以Gini(D)越小，数据集的纯度越高。
   - 分割后的gini依然使用加权的方法计算

## 剪枝

- 主要是为了对付过拟合
- 分为“预剪枝”和“后剪枝”
- **关键：**如何判断泛化性能是否提升？通过决策树在剪枝前后在验证集上的表现来判断

### 预剪枝

- 是什么：决策树生成的过程中，对每个节点，在划分前先进行评估，若当前节点划分不能带来决策树泛化性能的提升，则停止划分并把当前节点标记为叶节点
- 怎么做：对比当前节点划分前和划分后的两颗树在验证集上的表现，如果前者优于后者，则不进行划分
- 缺点：
  ![image-20181207154842863](/Users/yaosongding/Library/Application Support/typora-user-images/image-20181207154842863.png)

### 后剪枝

- 是什么：等到决策树生成完毕后，自底向上对非叶节点进行考察，若将该节点对应的子树替换为一个叶节点（相当于将这颗子树裁掉）可以带来泛化性能的提升，就进行裁剪

- 自底向上，依次考察每个非叶节点，时间开销比预剪枝大得多
- 作者还提到的一个**点**：当剪枝和不剪枝在验证集上的表现相同时，应该剪枝，利于泛化（奥卡姆剃刀准则）

## 连续与缺失值

### 连续值

- 一个具有连续值的属性出现了n个值，则可能有n - 1个划分点，对这些划分点划分后的结果分别计算信息增益，选取信息增益最大的划分点。（划分点可以是中位点，而作者举的例子选择了不大于中位点的最大样本点作为划分点）
- 需要注意的是，与离散的属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性，只不过划分的范围变成了各自的范围

### 缺失值

- 在实际应用中重要且常用，作者介绍了决策树处理缺失值的专有做法

- 需要解决的两个问题：

  1. 如何在属性缺失的情况下选择划分属性？
  2. 给定划分属性，样本缺失该属性，怎么办？

  上面的两个问题，p86 - p87给出了具体的解决方案。注意：初始时各权重都为1

## 多变量决策树

- 什么是**多变量**决策树？

  - 能实现“斜划分”或更复杂划分的决策树，作者在p90图文结合给出了定义。

  - 对每个非叶节点，学习一个线性分类器，至于具体的学习方法，作者并没有详细说明，但应该有很多办法，比如逻辑回归、svm等

- 作者画的这个图非常好，有利于形成对决策树的印象：

  ![image-20181208154152895](/Users/yaosongding/Library/Application Support/typora-user-images/image-20181208154152895.png)

## 阅读材料部分

- 经典的决策树算法：

  ID3、 C4.5、 CART

- 多变量决策树算法：OC1

- 可以“增量学习”的决策树算法：

  ![image-20181208160630939](/Users/yaosongding/Library/Application Support/typora-user-images/image-20181208160630939.png)

